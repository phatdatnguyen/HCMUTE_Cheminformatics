{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2d6e30",
   "metadata": {},
   "source": [
    "# Chapter 10. Machine Learning in Chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09f3a0",
   "metadata": {},
   "source": [
    "## 10.4. Machine Learning Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa334069",
   "metadata": {},
   "source": [
    "Evaluating a machine learning model is necessary to measure its performance and assess how well it can generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1a7ab",
   "metadata": {},
   "source": [
    "### 10.4.1. Bias versus Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60de145",
   "metadata": {},
   "source": [
    "Bias refers to the error introduced by approximating a real-world problem with a simplified model. It is the model's tendency to consistently make assumptions that deviate from the true relationship between input features and the target variable.\n",
    "\n",
    "Variance, on the other hand, refers to the model's sensitivity to fluctuations in the training data. It measures how much the predictions of the model vary for different training datasets.\n",
    "\n",
    "In summary:\n",
    "- Bias represents the model's systematic error or the assumptions it makes.\n",
    "- Variance represents the model's inconsistency or sensitivity to training data fluctuations.\n",
    "- **High bias** → **Underfit**\n",
    "- **High variance** → **Overfit**\n",
    "\n",
    "The bias-variance tradeoff:\n",
    "![Bias-variance tradeoff](images/bias_variance_tradeoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe2ce8",
   "metadata": {},
   "source": [
    "### 10.4.2. Splitting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84102ee",
   "metadata": {},
   "source": [
    "In order to evaluate a model, we have to split the dataset into 2 sets: train set and test set. The train dataset is used for training the model, and the test dataset is used for evaluation.\n",
    "![Train-test split](images/train_test_split.png)\n",
    "\n",
    "In training models with multiple iterations (e.g. ANN models). The dataset is usually splitted into 3 sets: train set, validation set, and test set. The validation dataset is used for monitoring the training process of the model in order to avoid underfitting and overfitting during training.\n",
    "![Train-val-test split](images/train_val_test_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d193f",
   "metadata": {},
   "source": [
    "### 10.4.3. Evaluation Metrics of Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd20d5",
   "metadata": {},
   "source": [
    "#### 10.4.3.1. Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61ccaf",
   "metadata": {},
   "source": [
    "![Regression metrics](images/regression_metrics.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a4b5c",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727551a",
   "metadata": {},
   "source": [
    "MAE measures the average absolute difference between the predicted and actual values, providing a linear scale of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a62f9b",
   "metadata": {},
   "source": [
    "**Mean Squared Error (MSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5fcbf",
   "metadata": {},
   "source": [
    "MSE measures the average squared difference between the predicted and actual values, giving higher weight to larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358f800",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error (RMSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57ced1",
   "metadata": {},
   "source": [
    "RMSE is the square root of MSE, providing an interpretable scale that matches the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5b0aa",
   "metadata": {},
   "source": [
    "**R<sup>2</sup> (Coefficient of Determination)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401981d",
   "metadata": {},
   "source": [
    "R<sup>2</sup> represents the proportion of the variance in the target variable that can be explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b053baf7",
   "metadata": {},
   "source": [
    "#### 10.4.3.2. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83384f",
   "metadata": {},
   "source": [
    "![Classification metrics](images/classification_metrics.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f234201",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6eb667",
   "metadata": {},
   "source": [
    "Confusion matrix Provides a tabular summary of the predicted versus actual class labels, allowing for the calculation of various metrics such as true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23824d7",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d9ee9",
   "metadata": {},
   "source": [
    "Accuracy measures the proportion of correctly classified instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9df11",
   "metadata": {},
   "source": [
    "**Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb0be6",
   "metadata": {},
   "source": [
    "Precision evaluates the ratio of true positive predictions to the total predicted positives, indicating the model's ability to correctly identify positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7ded2",
   "metadata": {},
   "source": [
    "**Recall (Sensitivity)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229a942",
   "metadata": {},
   "source": [
    "Recall (or sensitivity) measures the ratio of true positive predictions to the total actual positives, indicating the model's ability to correctly identify positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78290334",
   "metadata": {},
   "source": [
    "**F1-Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec303658",
   "metadata": {},
   "source": [
    "F1-score combines precision and recall into a single metric that considers both the false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332b345",
   "metadata": {},
   "source": [
    "## 10.5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcc2c5",
   "metadata": {},
   "source": [
    "We will explorer several common machine learning models that are available in `scikit-learn` library.\n",
    "\n",
    "To install `scikit-learn`, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fafe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4897d",
   "metadata": {},
   "source": [
    "### 10.5.1. Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1379196",
   "metadata": {},
   "source": [
    "In the following sections, will we evaluate different regression models to predict the lipophilicity of chemical compounds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8d16d",
   "metadata": {},
   "source": [
    "***a. Import modules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed281466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc47f2f",
   "metadata": {},
   "source": [
    "***b. Load dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19eccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the lipophilicity dataset\n",
    "data_file_path = '.\\\\datasets\\\\Lipophilicity.csv'\n",
    "df = pd.read_csv(data_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c711c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list of SMILES\n",
    "smiles_arr = df['smiles'].to_numpy()\n",
    "\n",
    "# Get the output\n",
    "y = df['lipophilicity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c0b4a",
   "metadata": {},
   "source": [
    "***c. Extraction features from molecules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31290df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to extract a list of features from a molecule\n",
    "def extract_features(mol):\n",
    "    features = []\n",
    "    features.append(Descriptors.MolWt(mol))          # molecular weight\n",
    "    features.append(Descriptors.NumHeteroatoms(mol)) # number of heteroatoms\n",
    "    features.append(Descriptors.RingCount(mol))      # number of rings\n",
    "    features.append(Descriptors.NumHAcceptors(mol))  # number of hydrogen bond donor\n",
    "    features.append(Descriptors.NumHDonors(mol))     # number of hydrogen bond accepter\n",
    "    features.append(Descriptors.FractionCSP3(mol))   # fraction of SP3-hybridized carbons\n",
    "    features.append(Descriptors.TPSA(mol))           # topological polar surface area\n",
    "    features.append(Descriptors.MolLogP(mol))        # partition coefficient\n",
    "    features.append(Descriptors.MolMR(mol))          # molar refractivity\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83cf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the list of features for molecules\n",
    "x = []\n",
    "\n",
    "# Loop through the SMILES list\n",
    "pbar = tqdm(range(len(smiles_arr)))\n",
    "for i in pbar:\n",
    "    # Get the SMILES for each molecule\n",
    "    smiles = smiles_arr[i]\n",
    "    \n",
    "    # Create a molecule object from the SMILES\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Get descriptors\n",
    "    features = extract_features(mol)\n",
    "    x.append(features)\n",
    "    \n",
    "    # Print progress\n",
    "    pbar.set_description('{}/{} molecules processed |'.format(i + 1, len(smiles_arr)))\n",
    "    \n",
    "# Convert list to numpy array\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3c1ec",
   "metadata": {},
   "source": [
    "***d. Data processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f84a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4b490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input and output scalers\n",
    "input_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the data\n",
    "x_scaled = input_scaler.fit_transform(x)\n",
    "y_scaled = output_scaler.fit_transform(y.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6854487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(x_scaled, y_scaled, test_size=0.3, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb215f5",
   "metadata": {},
   "source": [
    "***e. Model training and evaluation***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483dbab1",
   "metadata": {},
   "source": [
    "#### 10.5.1.1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fa7b7",
   "metadata": {},
   "source": [
    "Linear regression is a fundamental approach that models the linear relationship between a dependent variable and one or more independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd7c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b707bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8c86f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a82e4",
   "metadata": {},
   "source": [
    "After training, we can make predictions with this model, for examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadf27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a new molecule\n",
    "mol = Chem.MolFromSmiles('CCO') # ethanol\n",
    "\n",
    "# Extract features\n",
    "features = extract_features(mol)\n",
    "features = np.array(features)\n",
    "\n",
    "# Scale features\n",
    "features_scaled = input_scaler.transform(features.reshape(1, -1))\n",
    "\n",
    "# Make prediction\n",
    "output_scaled = model.predict(features_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "output = output_scaler.inverse_transform(output_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Print out the prediction\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ae1a8",
   "metadata": {},
   "source": [
    "#### 10.5.1.2. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b0db1",
   "metadata": {},
   "source": [
    "Ridge regression extends linear regression by adding a regularization term, which helps in reducing model complexity and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d3991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize the Ridge Regression model\n",
    "# You can adjust the alpha parameter to control the amount of regularization\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b8829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9a464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa585087",
   "metadata": {},
   "source": [
    "#### 10.5.1.3. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708bc9e",
   "metadata": {},
   "source": [
    "Lasso regression, similar to ridge regression, adds a regularization term but in a way that can completely eliminate the weights of some features, thus performing feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f9b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize the Lasso Regression model\n",
    "# You can adjust the alpha parameter to control the amount of regularization\n",
    "model = Lasso(alpha=0.01)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03e7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2438a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ec1aa",
   "metadata": {},
   "source": [
    "#### 10.5.1.4. Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d3799",
   "metadata": {},
   "source": [
    "Elastic net combines features of both ridge and lasso regression, using a mix of both L1 and L2 regularization to improve model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b070d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize the Elastic Net model\n",
    "# You can adjust the alpha and l1_ratio parameters to control the amount of regularization\n",
    "# alpha controls the overall strength, while l1_ratio controls the balance between L1 and L2 regularization\n",
    "model = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345303f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01455412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbb72a",
   "metadata": {},
   "source": [
    "#### 10.5.1.5. K-Nearest Neighbors Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cb7f8",
   "metadata": {},
   "source": [
    "KNN regression predicts the output based on the K nearest neighbors in the feature space, averaging their values to determine the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8970b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize the KNN regression model\n",
    "# You can adjust the number of neighbors (n_neighbors)\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ede8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7eb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f57c1a",
   "metadata": {},
   "source": [
    "#### 10.5.1.6. Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779f2f0",
   "metadata": {},
   "source": [
    "Decision tree regression models make predictions by splitting data into subsets based on feature values, building a tree-like model of decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058c40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the Decision Tree Regression model\n",
    "# You can adjust various parameters like max_depth, min_samples_split, etc.\n",
    "model = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70443d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196df4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2f6616",
   "metadata": {},
   "source": [
    "#### 10.5.1.7. Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec072d7",
   "metadata": {},
   "source": [
    "Random forest regression improves upon decision tree regression by creating an ensemble of decision trees and averaging their predictions to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715cb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest Regression model\n",
    "# You can adjust parameters like n_estimators (number of trees), max_depth, etc.\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4222d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd2fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c6534",
   "metadata": {},
   "source": [
    "#### 10.5.1.8. Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f35e0",
   "metadata": {},
   "source": [
    "Gaussian process regression is a probabilistic model that uses kernel functions to make predictions, providing not only estimations but also uncertainty measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6142e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "# Initialize the Gaussian Process Regressor model\n",
    "# You can adjust the kernel and other parameters as needed\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "model = GaussianProcessRegressor(kernel=kernel, random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e33f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6630db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26834db",
   "metadata": {},
   "source": [
    "#### 10.5.1.9. Support Vector Machine (SVM) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3e363",
   "metadata": {},
   "source": [
    "SVM regression, or Support Vector Regression (SVR), uses the SVM technique to model complex relationships between features and target variables, including both linear and non-linear interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cce5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize the SVM Regression model with a Gaussian (RBF) kernel\n",
    "# You can adjust parameters like C (regularization parameter) and gamma (kernel coefficient)\n",
    "model = SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.1)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(x_test_scaled)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Transform the test set back to original scale\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c12b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b9cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=40)\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2)  # Ideal line for perfect predictions\n",
    "plt.xlabel('Actual Lipophilicity')\n",
    "plt.ylabel('Predicted Lipophilicity')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9fbd8",
   "metadata": {},
   "source": [
    "### 10.5.2. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc8978",
   "metadata": {},
   "source": [
    "In the following sections, will we evaluate different regression models to predict whether a compound is active toward β-secretase inhibitors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad65cd1",
   "metadata": {},
   "source": [
    "***a. Import modules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e33c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f221962",
   "metadata": {},
   "source": [
    "***b. Load dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ba434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the lipophilicity dataset\n",
    "data_file_path = '.\\\\datasets\\\\BBBP.csv'\n",
    "df = pd.read_csv(data_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list of SMILES\n",
    "smiles_arr = df['smiles'].to_numpy()\n",
    "\n",
    "# Get the output\n",
    "y = df['p_np'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a8d5f",
   "metadata": {},
   "source": [
    "***c. Extraction features from molecules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09132a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to extract a list of features from a molecule\n",
    "def extract_features(mol):\n",
    "    morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048) # Morgan fingerprint\n",
    "    features = np.array(morgan_fp)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7d7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the list of features for molecules\n",
    "x = []\n",
    "output = []\n",
    "\n",
    "# Loop through the SMILES list\n",
    "pbar = tqdm(range(len(smiles_arr)))\n",
    "for i in pbar:\n",
    "    # Get the SMILES for each molecule\n",
    "    smiles = smiles_arr[i]\n",
    "    \n",
    "    # Create a molecule object from the SMILES\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Get descriptors\n",
    "    try:\n",
    "        features = extract_features(mol)\n",
    "        x.append(features)\n",
    "        output.append(y[i])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Print progress\n",
    "    pbar.set_description('{}/{} molecules processed |'.format(i + 1, len(smiles_arr)))\n",
    "    \n",
    "# Convert list to numpy array\n",
    "x = np.array(x)\n",
    "y = np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2e44c",
   "metadata": {},
   "source": [
    "***d. Data processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f590f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cb56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply a variance threshold to remove input columns with only one value.\n",
    "var_thresholder = VarianceThreshold(threshold=0.01)\n",
    "x_var_thresh = var_thresholder.fit_transform(x)\n",
    "\n",
    "# Define input scaler and scale input data\n",
    "input_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_scaled = input_scaler.fit_transform(x_var_thresh)\n",
    "\n",
    "# Apply PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=32)\n",
    "x_pca = pca.fit_transform(x_var_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1f25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train_scaled, x_test_scaled, y_train, y_test = train_test_split(x_pca, y, test_size=0.3, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72bd49",
   "metadata": {},
   "source": [
    "***e. Model training and evaluation***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa336e",
   "metadata": {},
   "source": [
    "#### 10.5.2.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae6c3b",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although there are extensions to handle multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f7009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "# You can adjust the 'C' parameter to control regularization strength\n",
    "model = LogisticRegression(C=1.0, random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516e726",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88a7f2",
   "metadata": {},
   "source": [
    "#### 10.5.2.2. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349c4cc",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong independence assumptions between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9ecc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07287118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1dfea",
   "metadata": {},
   "source": [
    "#### 10.5.2.3. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322a268",
   "metadata": {},
   "source": [
    "KNN classification predicts the class of a data point based on the majority class among its k nearest neighbors. It's a simple, distance-based algorithm often used for its ease of interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062890d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "# You can adjust the number of neighbors (n_neighbors)\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79721756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17029db2",
   "metadata": {},
   "source": [
    "#### 10.5.2.4. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c741595",
   "metadata": {},
   "source": [
    "Decision tree classifiers make decisions by splitting data based on feature values, creating a tree-like model of decisions. They are intuitive and easy to interpret but can be prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656632ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "# You can adjust parameters like max_depth, min_samples_split, etc.\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1bce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3a653",
   "metadata": {},
   "source": [
    "#### 10.5.2.5. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a5b99",
   "metadata": {},
   "source": [
    "Random forest classifiers improve upon decision trees by creating an ensemble of decision trees and aggregating their predictions to reduce overfitting and improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adc4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "# You can adjust parameters like n_estimators (number of trees), max_depth, etc.\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba280efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e52069",
   "metadata": {},
   "source": [
    "#### 10.5.2.6. Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48c2c0",
   "metadata": {},
   "source": [
    "Gaussian Process classifiers extend Gaussian processes to classification tasks, using kernel functions and Bayesian inference to predict categorical outcomes, often with uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c70a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Initialize the Gaussian Process Classifier\n",
    "# The choice of kernel can be important; RBF is a common choice\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "model = GaussianProcessClassifier(kernel=kernel, random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a223bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fadf7",
   "metadata": {},
   "source": [
    "#### 10.5.2.7. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e515736",
   "metadata": {},
   "source": [
    "SVM classifiers construct hyperplanes in a multidimensional space to separate different classes with as wide a margin as possible. SVMs are effective in high-dimensional spaces and versatile with various kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21de001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM classifier with a Gaussian (RBF) kernel\n",
    "# You can adjust parameters like C (regularization parameter) and gamma (kernel coefficient)\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=random_seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31594000",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd79b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
